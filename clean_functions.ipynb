{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "Initial functions: \n",
    "* read_data: reads data and returns X and y\n",
    "* transform_data: standarizes the features X to be in the same scale\n",
    "\n",
    "Dimensionality reduction + Propensity scores:\n",
    "* factor_analysis: reduces the dimension of X to a smaller number of factors\n",
    "* interpret_factors: for each factor plots the loadings of the most important features composing it\n",
    "* logistic_regression: computes propensity scores for each unit\n",
    "* ps_balanced: repeatedly computes propensity scores with random balanced subsamples\n",
    "\n",
    "Feature selection:\n",
    "* feature_selection: selects subset of important features with sparisity and multiconinearity constraints\n",
    "\n",
    "Distance matrix:\n",
    "* distance_matrix: computes the matrix distance for all combinations of treatment and control pairs\n",
    "* distance_matrix_hard_constraints: computes the matrix distance for all combinations of treatment and control pairs including penalty for hard constraint features\n",
    "* ps_distance_matrix: computes the matrix distance for all combinations of treatment and control pairs considering only propensity scores\n",
    "\n",
    "Matching algorithms:\n",
    "* dist_matching: matching based on distance or propensity scores only\n",
    "* dist_matching_PS_calipers: matching based on distance with propensity scores calipers\n",
    "* dist_matching_PS_calipers_HC: matching based on distance with propensity scores calipers including hard constraints in the optimization for better time efficiency\n",
    "\n",
    "Evaluation:\n",
    "* asmd: computes the average standardized mean difference between treatment and control groups\n",
    "* bias_matches: computes the bias among features for the matches\n",
    "* ate: computes the average treatment effect of a matched data\n",
    "* variance_matches: calculates the variance among matches\n",
    "* variance_matches: plots the bias distribution for each feature\n",
    "* bias_boxplot: plots the cumulative distribution of bias values for each feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path='../Data/Final/CLEAN_DATA_22_Imputed.csv', outcome='STAAR_Meets', treatment='SURVEY_CRIMSI'):\n",
    "    \"\"\"\n",
    "    Read in data and split into covariates, treatment, and outcome.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to data.\n",
    "    outcome : str\n",
    "        Name of outcome variable.\n",
    "    treatment : str\n",
    "        Name of treatment variable.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    X : pd.DataFrame\n",
    "        Covariates.\n",
    "    T : pd.Series\n",
    "        Treatment.\n",
    "    y : pd.Series\n",
    "        Outcome.\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(path)  #load data\n",
    "    y = data[outcome] #define outcome variable\n",
    "    T = data[treatment] #define treatment variable\n",
    "    X = data.drop([outcome, treatment], axis=1) #define covariates\n",
    "    return X, T, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(X):\n",
    "    \"\"\"\n",
    "    Transform data by standardizing numerical variables, binarizing binary variables, and one-hot encoding categorical variables.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        The feature matrix.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    X_trans: pd.DataFrame\n",
    "        The standardized feature matrix.\n",
    "    \"\"\"\n",
    "     \n",
    "    bin_cols = [col for col in X.columns if set(X[col]) == {0, 1}] #define binary variables\n",
    "    cat_cols = [col for col in X.columns if X[col].dtype == 'object' or X[col].nunique() < 8 and col not in bin_cols] #define categorical variables\n",
    "    num_cols = [col for col in X.columns if col not in bin_cols + cat_cols] #define numerical variables\n",
    "\n",
    "    # Define the column transformer to standardize numerical, binary, and categorical variables separately\n",
    "    ct = ColumnTransformer([ ('num', StandardScaler(), num_cols), ('bin', Binarizer(), bin_cols), ('cat', OneHotEncoder(drop='first'), cat_cols)])\n",
    "\n",
    "    # Fit the column transformer to the training data\n",
    "    ct.fit(X)\n",
    "\n",
    "    # Get the feature names after transformation (for interpretability)\n",
    "    feature_names = num_cols + bin_cols + list(ct.named_transformers_['cat'].get_feature_names_out(cat_cols))\n",
    "\n",
    "    # Transform the data and create a new DataFrame with the transformed features \n",
    "    X_trans = pd.DataFrame(ct.transform(X), columns=feature_names)\n",
    "\n",
    "    return X_trans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction + Propensity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_analysis(X, T, k=None):\n",
    "    \"\"\"\n",
    "    Reduce the dimensionality of X using Factor Analysis to k number of factors.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        The feature matrix.\n",
    "    T : pd.Series\n",
    "        The treatment vector.\n",
    "    k : int\n",
    "        The number of factors to keep.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    X_factor : pd.DataFrame\n",
    "        The factor matrix.\n",
    "    fa : FactorAnalyzer\n",
    "        The fitted FactorAnalyzer object.\n",
    "    \"\"\"\n",
    "\n",
    "    # if k (# of factors) is not given, define k with the 1:10 rule to avoid overfitting which states that k should be 10% of the treatment size. Ex: if treatment size is 100, k = 10\n",
    "    if k is None:\n",
    "        k = int(np.floor(X[T==1].shape[0]/10)) # define k with the 1:10 rule\n",
    "    \n",
    "    fa = FactorAnalyzer(n_factors=k, rotation='varimax') # define FactorAnalyzer object\n",
    "    fa.fit(X) # fit FactorAnalyzer object to features space X\n",
    "    X_factor = fa.transform(X) # transform X to factor space\n",
    "    X_factor = pd.DataFrame(X_factor).rename(columns=lambda x: 'Factor_' + str(x+1)) # create a new DataFrame with the factor space features\n",
    "    return X_factor, fa # return the data with factors and the fitted FactorAnalyzer object for later use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_factors(X,T):\n",
    "    \"\"\"\n",
    "    Interpret the factors from the factor analysis by plotting the loadings of the top 5 features for each factor.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        The feature matrix.\n",
    "    T : pd.Series\n",
    "        The treatment vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_factor = factor_analysis(X, T)[1] # Run factor analysis \n",
    "\n",
    "    loadings = X_factor.loadings_ # Get the loadings of the factors \n",
    "\n",
    "    # Create a dataframe of the loadings \n",
    "    loadings_df=pd.DataFrame.from_records(loadings)\n",
    "    loadings_df.columns = ['Factor_' + str(i+1) for i in range(loadings_df.shape[1])] # Rename the columns to Factor_1, Factor_2, etc.\n",
    "    loadings_df.index = X.columns # Rename the rows to the feature names for interpretability\n",
    "\n",
    "    # Plot for each factor the loadings of top 5 features contributing to the factor\n",
    "    plt.rcParams['figure.figsize'] = [4, 4]\n",
    "    for i in range(loadings_df.shape[1]):\n",
    "        loadings_df.iloc[:,i].abs().sort_values(ascending=False)[:5].plot(kind='barh')\n",
    "        plt.title('Factor ' + str(i+1))\n",
    "        plt.ylabel('Loading')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, T):\n",
    "    \"\"\"\n",
    "    Compute propensity scores using logistic regression. Propensity scores are the probability of each row to belong to the treatment group.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        The feature matrix.\n",
    "    T : pd.Series\n",
    "        The treatment vector.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    propenity_scores : pd.Series\n",
    "        The propensity scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # fit logistic regression model\n",
    "    log_reg = LogisticRegression(max_iter=1000, random_state=0).fit(X, T)\n",
    "\n",
    "    # predict probabilities for each row to belong to treatment group\n",
    "    probs = log_reg.predict_proba(X)[:,1] \n",
    "\n",
    "    #convert probs to Series with index of X\n",
    "    propensity_scores=pd.Series(probs, index=X.index)\n",
    "    \n",
    "    return propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ps_balanced(X, T):\n",
    "    \"\"\"\n",
    "    Repeatedly compute propensity scores with random subsamples of the control group to solve the class imbalance problem between treated and control.\n",
    "    We repeat this process until all control units have a propensity score, and we average all the computed scores for the treated units to get the final propensity scores.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        The feature matrix.\n",
    "    T : pd.Series\n",
    "        The treatment vector.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    ps_df : pd.DataFrame\n",
    "        The propensity scores for each subsample.\n",
    "    propensity_scores_final : pd.Series\n",
    "        The final propensity scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define control and treated groups\n",
    "    control = X[T == 0]\n",
    "    treated = X[T == 1]\n",
    "    \n",
    "    # Determine the number of subsamples to create. Ex: if control size is 100 and treated size is 10, we need 10 subsamples of control units \n",
    "    n_subsamples = int(np.ceil(control.shape[0] / treated.shape[0]))\n",
    "\n",
    "    # Create a dataframe to store the propensity scores for each unit\n",
    "    propensity_score_cols = [f'propensity_score_{i}' for i in range(n_subsamples)] # Create a list to store the propensity score columns\n",
    "    ps_df = pd.DataFrame(columns=propensity_score_cols, index=X.index)\n",
    "    \n",
    "    # Randomly shuffle the control units to avoid bias in the subsamples\n",
    "    shuffled_control = control.sample(frac=1, random_state=42)\n",
    "    \n",
    "    # Loop to compute propensity scores for each subsample\n",
    "    for i in range(n_subsamples): # Loop over the number of subsamples\n",
    "        # Calculate the start and end indices for the current subsample\n",
    "        start_idx = i * treated.shape[0]  \n",
    "        end_idx = min((i + 1) * treated.shape[0], control.shape[0]) # Make sure that the last subsample contains all the remaining control units\n",
    "        \n",
    "        control_sample = shuffled_control.iloc[start_idx:end_idx] # Get the control units for the current subsample\n",
    "        \n",
    "        sample = pd.concat([control_sample, treated], axis=0) # Combine the control sample and the treated group units to compute propensity scores\n",
    "        \n",
    "        sample_ps = logistic_regression(sample, T.loc[sample.index]) # Compute propensity scores for the sample\n",
    "        \n",
    "        # Append the propensity scores series to the dataframe\n",
    "        ps_df.loc[sample_ps.index, f'propensity_score_{i}'] = sample_ps\n",
    "    \n",
    "    # Compute the final propensity score as the mean of the subsample propensity scores \n",
    "    ps_df['FINAL_PS'] = ps_df.mean(axis=1) \n",
    "\n",
    "    # Convert final propensity scores as a series with index of X\n",
    "    propensity_scores_final=pd.Series(ps_df['FINAL_PS'], index=X.index)\n",
    "\n",
    "    return ps_df, propensity_scores_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(X, y, cv=5, method='lasso', num_selected=15, max_correlation=0.6, must_include=None):\n",
    "    \"\"\"\n",
    "    Performs feature selection using Lasso, Ridge, or ElasticNet with optimization and adding multicollinearity and sparsity constraints.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        The feature matrix.\n",
    "    y : pd.Series\n",
    "        The outcome vector.\n",
    "    cv : int\n",
    "        Number of folds for cross-validation.\n",
    "    method : str\n",
    "        The method to use for feature selection. Must be 'lasso', 'ridge', or 'elastic'.\n",
    "    num_selected : int\n",
    "        The number of features to select.\n",
    "    max_correlation : float\n",
    "        The maximum correlation allowed between two features.\n",
    "    must_include : list, optional\n",
    "        The list of features that must be included in the matching model.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    selected_features : list\n",
    "        The list of selected features names.\n",
    "    coef_all : list\n",
    "        The list of coefficients of all features.\n",
    "    coef_selected : list\n",
    "        The list of coefficients of selected features.\n",
    "\n",
    "    \"\"\"\n",
    "    num_features = X.shape[1]\n",
    "\n",
    "    # Define the model\n",
    "    if method == 'lasso':\n",
    "        model = LassoCV(cv=cv)\n",
    "    elif method == 'ridge':\n",
    "        model = RidgeCV(cv=cv)\n",
    "    elif method == 'elastic':\n",
    "        model = ElasticNetCV(cv=cv)\n",
    "    else:\n",
    "        raise ValueError('Method must be \"lasso\", \"ridge\", or \"elastic\"')\n",
    "\n",
    "    # Fit the model to the data\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Get the optimal value of alpha and the corresponding coefficients\n",
    "    alpha = model.alpha_\n",
    "    coef = model.coef_\n",
    "\n",
    "    # Create the optimization problem to include the sparsity and multicollinearity constraints\n",
    "    prob = LpProblem(\"Feature_Selection\", LpMaximize) # Define the optimization as a maximization problem\n",
    "\n",
    "    # Create binary decision variables for each feature. If the feature is selected, the variable is 1, otherwise it is 0\n",
    "    selected = LpVariable.dicts(\"selected\", range(num_features), cat=\"Binary\") \n",
    "\n",
    "    # Set the objective function: maximize the sum of absolute selected coefficients \n",
    "    prob += lpSum([selected[i] * abs(coef[i]) for i in range(num_features)])\n",
    "\n",
    "    # Add constraints\n",
    "    # Constraint 1: Avoid multicollinearity (correlation threshold)\n",
    "    for i in range(len(X.columns)):\n",
    "        for j in range(i + 1, len(X.columns)):\n",
    "            if abs(pearsonr(X.iloc[:, i], X.iloc[:, j])[0]) > max_correlation: # If the correlation between the two features is greater than the threshold\n",
    "                prob += selected[i] + selected[j] <= 1 # We can only select one of the two features to avoid multicollinearity\n",
    "\n",
    "    # Constraint 2: Limit the number of selected features (sparsity constraint)\n",
    "    prob += lpSum([selected[i] for i in range(len(X.columns))]) <= num_selected\n",
    "\n",
    "    # Constraint 3: Must include certain features\n",
    "    if must_include is not None:\n",
    "        for feature in must_include:\n",
    "            # Get the index of the feature and set x to 1 (selected)\n",
    "            feature_index = X.columns.tolist().index(feature)\n",
    "            prob += selected[feature_index] == 1\n",
    "\n",
    "\n",
    "    # Solve the problem and don't show the output\n",
    "    prob.solve(PULP_CBC_CMD(msg=0))\n",
    "\n",
    "    selected_indices = [i for i in range(num_features) if selected[i].varValue == 1] # Get the selected feature indices\n",
    "    selected_features = X.columns[selected_indices].tolist() # Get the selected feature names\n",
    "\n",
    "    # Save coefficients of all features in a list\n",
    "    coef_all = coef.tolist()\n",
    "\n",
    "    # Save coefficients of selected features in a list\n",
    "    coef_selected = [coef[i] for i in selected_indices]\n",
    "\n",
    "    return selected_features, coef_all, coef_selected # Return the selected features names, the coefficients of all features, and the coefficients of selected features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix(X, T, features=None, weights=None):\n",
    "    \"\"\"\n",
    "    Calculate the weighted (absolute euclidean) distance matrix between treated and control units for the features specified.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pandas.DataFrame\n",
    "        The feature matrix.\n",
    "    T : pd.series\n",
    "        Series with treatment indicator. 1 if treatment, 0 if control.\n",
    "    features : list, optional\n",
    "        The list of features to be used for distance calculation (default: None). If None, all features are used.\n",
    "    weights : list, optional\n",
    "        The list of weights to be used for distance calculation (default: None). If None, features are not weighted.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    D : pandas.DataFrame\n",
    "        The distance matrix where rows correspond to treated units and columns correspond to control units. \n",
    "        The row and column names are the indexes of the original data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Define treated and control units\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "\n",
    "    # Define subset of features to be used for distance calculation\n",
    "    if features is None: # If no features are specified, use all features\n",
    "        features = X.columns\n",
    "    \n",
    "    # Define weights for each feature\n",
    "    if weights is None: # If no weights are specified, use equal weights for all features\n",
    "        weights = np.ones(len(features))\n",
    "\n",
    "    # Select the values of the relevant features from treated and control units\n",
    "    treated_features = treated[features].values\n",
    "    control_features = control[features].values\n",
    "\n",
    "    # Calculate the weighted distance matrix using vectorized operations\n",
    "    weighted_diff = np.abs((treated_features[:, np.newaxis, :] - control_features) * weights).sum(axis=2) # Absolute weighted difference between treated and control units for each feature\n",
    "\n",
    "    # Transform into a DataFrame and add row and column names as indexes of the original data\n",
    "    D = pd.DataFrame(weighted_diff, index=treated.index, columns=control.index)\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix_hard_constraints(X, T, features=None, hard_constraint_features=None, weights=None, penalty=1e9):\n",
    "    \"\"\"\n",
    "    Calculate the weighted (absolute euclidean) distance matrix between treated and control units for the features specified,\n",
    "    adding penalty for hard constraints features that don't have the same value between treated and control units.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pandas.DataFrame\n",
    "        The feature matrix.\n",
    "    T : pd.series\n",
    "        Series with treatment indicator. 1 if treatment, 0 if control.\n",
    "    features : list, optional\n",
    "        The list of features to be used for distance calculation (default: None). If None, all features are used.\n",
    "    weights : list, optional\n",
    "        The list of weights to be used for distance calculation (default: None). If None, features are not weighted.\n",
    "    hard_constraint_features : list, optional\n",
    "        The list of features to be used for distance calculation with hard constraints (default: None). If None, no features have hard constraints.\n",
    "    weights: list, optional\n",
    "        The list of weights to be used for distance calculation (default: None). If None, features are not weighted.\n",
    "    penalty : float, optional\n",
    "        The penalty to be applied to the distance between treated and control units for features with hard constraints. If penalty is higher, we ensure features with hard constraints are closer.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    D : pandas.DataFrame\n",
    "        The distance matrix where rows correspond to treated units and columns correspond to control units.\n",
    "        The row and column names are the indexes of the original data.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define treated and control units\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "\n",
    "    # Define subset of features to be used for distance calculation\n",
    "    if features is None: # If no features are specified, use all features\n",
    "        features = X.columns.tolist()\n",
    "    else:\n",
    "        features = features.copy() # Make a copy of the list of features to avoid modifying the original list\n",
    "\n",
    "    # Check if hard constraint features are present in the list of features\n",
    "    if hard_constraint_features is not None:\n",
    "        missing_features = set(hard_constraint_features) - set(features) # Get the list of hard constraint features that are not in the list of features\n",
    "        if missing_features: \n",
    "            features += missing_features # Add missing hard constraint features to the list of features\n",
    "    \n",
    "    # Define weights for each feature\n",
    "    if weights is None: # If no weights are specified, use equal weights for all features\n",
    "        weights = np.ones(len(features))\n",
    "    elif len(weights) != len(features): # If the number of weights is not equal to the number of features, add weights neutral weights for the hard constraints features\n",
    "        missing_weights = np.ones(len(features) - len(weights))\n",
    "        weights = np.concatenate([weights, missing_weights])\n",
    "\n",
    "    # Select the values of the relevant features from treated and control units\n",
    "    treated_features = treated[features].values\n",
    "    control_features = control[features].values\n",
    "\n",
    "    # Calculate the abolsute distances for all features\n",
    "    distances = np.abs((treated_features[:, np.newaxis, :] - control_features) * weights)\n",
    "\n",
    "    # Apply penalty to features with hard constraints\n",
    "    if hard_constraint_features is not None:\n",
    "        hard_constraint_indices = [features.index(feature) for feature in hard_constraint_features] # Get the indices of the hard constraint features\n",
    "        distances[:, :, hard_constraint_indices] *= penalty # Apply penalty to the distances of the hard constraint features\n",
    "    \n",
    "    # Sum the distances across features to get the final distance matrix\n",
    "    D = np.sum(distances, axis=2)\n",
    "\n",
    "    # Transform into a DataFrame and add row and column names as indexes of the original data\n",
    "    D = pd.DataFrame(D, index=treated.index, columns=control.index)\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ps_distance_matrix(T, PS):\n",
    "    \"\"\"\n",
    "    Calculate the weighted (absolute euclidean) distance matrix between treated and control units for the propensity scores.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    T : pd.series\n",
    "        Series with treatment indicator. 1 if treatment, 0 if control.\n",
    "    PS : pd.series\n",
    "        Series with propensity scores.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    D : pandas.DataFrame\n",
    "        The distance matrix of propensity scores where rows correspond to treated units and columns correspond to control units. \n",
    "        The row and column names are the indexes of the original data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Define treated and control units propensity scores\n",
    "    treated_ps = PS.loc[T[T == 1].index]\n",
    "    control_ps = PS.loc[T[T == 0].index]\n",
    "    \n",
    "    # Calculate the absolute distance matrix using vectorized operations\n",
    "    ps_diff = np.abs(treated_ps.values[:, np.newaxis] - control_ps.values)\n",
    "\n",
    "    # Transform into a DataFrame and add row and column names as indexes of the original data\n",
    "    D = pd.DataFrame(ps_diff, index=treated_ps.index, columns=control_ps.index)\n",
    "\n",
    "    return D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_matching(X, T, dist, with_replacement=True, optimal=False, num_matches=1):\n",
    "    \"\"\"\n",
    "    This function takes in a dataframe with treatment and control units, a dataframe with distances between treatment and control units, and a matching type\n",
    "    and returns a dataframe with matched units.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.dataframe\n",
    "        Feature matrix.\n",
    "    T : pd.series\n",
    "        Series with treatment indicator. 1 if treatment, 0 if control.\n",
    "    dist : pd.dataframe\n",
    "        Dataframe with distances between treatment (rows) and control units (columns).\n",
    "    with_replacement : boolean\n",
    "        Whether or not to match with replacement. \n",
    "    optimal : boolean\n",
    "        Whether or not to use optimal matching.\n",
    "    num_matches : int\n",
    "        Number of control matches per treatment unit. If 1, one-to-one matching is performed. If >1, one-to-many matching is performed.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    match_full : dataframe\n",
    "        Dataframe with matched units.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify treatment and control units\n",
    "    treatment = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "\n",
    "    matches = [] # Create an empty list to store matches\n",
    "\n",
    "    # -------- OPTION 1: WITH REPLACEMENT ----------\n",
    "    if with_replacement:\n",
    "        match_num = 1 # Initialize the match number\n",
    "        for t_idx, t_row in treatment.iterrows(): # Iterate over the treatment units\n",
    "            match_indices = dist.loc[t_idx][control.index].nsmallest(num_matches).index.tolist() # Get the indices of the k closest control units\n",
    "            matches.extend([[t_idx, c_idx, match_num] for c_idx in match_indices]) # Add the matches to the matches list\n",
    "            match_num += 1 # Increment the match number\n",
    "    \n",
    "    # -------- OPTION 2: WITHOUT REPLACEMENT ----------\n",
    "    elif not with_replacement:\n",
    "        # --------- OPTION 2A: GREEDY MATCHING ----------\n",
    "        if not optimal:\n",
    "            match_num = 1 # Initialize the match number\n",
    "            matched_controls = set()  # Keep track of matched control units\n",
    "            for t_idx, t_row in treatment.iterrows():  # Iterate over the treatment units\n",
    "                available_controls = [c_idx for c_idx in control.index if c_idx not in matched_controls]\n",
    "                control_indices = dist.loc[t_idx][available_controls].nsmallest(num_matches).index.tolist()\n",
    "                matches.extend([[t_idx, c_idx, match_num] for c_idx in control_indices])\n",
    "                matched_controls.update(control_indices)\n",
    "                match_num += 1 # Increment the match number\n",
    "\n",
    "        # --------  OPTION 2B: OPTIMAL MATCHING (OPTIMIZATION) ----------\n",
    "        elif optimal:\n",
    "            # Create optimization problem: minimization problem\n",
    "            prob = pulp.LpProblem(\"Matching_Problem\", pulp.LpMinimize)\n",
    "\n",
    "            # Define decision variables: binary variable x_{ij} = 1 if treatment unit i is matched to control unit j, 0 otherwise\n",
    "            x = pulp.LpVariable.dicts(\"x\", [(t_idx, c_idx) for t_idx in treatment.index for c_idx in control.index], cat='Binary')\n",
    "\n",
    "            # Objective function: minimize total distance\n",
    "            prob += pulp.lpSum(dist.loc[t_idx][c_idx] * x[(t_idx, c_idx)] for t_idx in treatment.index for c_idx in control.index)\n",
    "\n",
    "            # Constraint 1: each treatment unit is matched to at least the number of control specified\n",
    "            for t_idx in treatment.index: \n",
    "                    prob += pulp.lpSum(x[(t_idx, c_idx)] for c_idx in control.index) >= num_matches  \n",
    "\n",
    "            # Constraint 2: each control unit is matched to at most one treatment unit\n",
    "            for c_idx in control.index:\n",
    "                prob += pulp.lpSum(x[(t_idx, c_idx)] for t_idx in treatment.index) <= 1\n",
    "\n",
    "            # Solve the optimization problem\n",
    "            start_time = time.time()  # Record the start time\n",
    "            prob.solve(pulp.GLPK_CMD(msg=0)) # Use GLPK solver and suppress output\n",
    "            end_time = time.time()  # Record the end time\n",
    "            execution_time = end_time - start_time # Calculate the execution time\n",
    "            print(\"Execution Time:\", execution_time, \"seconds\")\n",
    "\n",
    "            # Retrieve the matches from the decision variables \n",
    "            match_num = 1  # Initialize the match number\n",
    "            matched_treatment = set()  # Keep track of treatment units that have been matched\n",
    "            for t_idx in treatment.index: # Iterate over the treatment units\n",
    "                matched_control = []  # Keep track of control units matched with this specific treatment unit\n",
    "                for c_idx in control.index: # Iterate over the control units\n",
    "                    if pulp.value(x[(t_idx, c_idx)]) == 1: # If the decision variable is 1, the pair is a match\n",
    "                        matched_control.append(c_idx) # Add the control unit index to the matched control list\n",
    "                if matched_control: # If the matched control list is not empty\n",
    "                    if t_idx not in matched_treatment: # If the treatment unit has not been matched yet, it is not in the set\n",
    "                        for c_idx in matched_control: # Iterate over the matched control units to this specific treatment unit\n",
    "                            matches.append([t_idx, c_idx, match_num]) # Add the matched pair to the matches list with the match number\n",
    "                        matched_treatment.add(t_idx) # Add the treatment unit index to the matched treatment set to keep track of matched treatment units\n",
    "                        match_num += 1 # Increment the match number\n",
    "    \n",
    "    # If the problem found matches then return the matches\n",
    "    if matches:\n",
    "        match_full = pd.DataFrame(columns=X.columns)  # Create an empty dataframe to store the matches including the features from X\n",
    "\n",
    "        # Iterate over the matches and include the matched number column \n",
    "        for match in matches:\n",
    "            match_treatment = treatment.loc[match[0]].to_frame().T # Get matched treatment unit features values from X\n",
    "            match_control = control.loc[match[1]].to_frame().T # Get matched control unit features values from X\n",
    "            match_treatment['match_num'] = match[2]  # Use the match number from the matches list\n",
    "            match_control['match_num'] = match[2]  # Use the match number from the matches list\n",
    "            match_full = pd.concat([match_full, match_treatment, match_control]) # Concatenate the matched treatment and control units to the match_full dataframe\n",
    "                \n",
    "        # Add a column for the treatment indicator from T\n",
    "        match_full['treatment'] = T[match_full.index]\n",
    "\n",
    "        #drop duplicated treatment rows that are added for when num_matches>1 (just for formatting purposes)\n",
    "        match_full=match_full[~((match_full['treatment']==1) & (match_full['match_num'].duplicated()))]\n",
    "        return match_full\n",
    "    \n",
    "    # If the problem did not find matches, then print a suggestion to relax the constraints\n",
    "    elif not matches:\n",
    "        print('** WARNING ** No matches found. You should try to relax your constraints in the optimization problem as the problem may be infeasible. Also, you can try to reduce the number of control matches as there might not be enough control units to match to.')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_matching_PS_calipers(X, T, PS, dist, with_replacement=True, optimal=False, num_matches=1, caliper=.02):\n",
    "    \"\"\"\n",
    "    Matches treated and control units based on the distance matrix and propensity scores with calipers.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pandas.DataFrame\n",
    "        The feature matrix.\n",
    "    T : pandas.Series\n",
    "        Series with treatment indicator. 1 if treatment, 0 if control.\n",
    "    PS : pandas.Series\n",
    "        Series with propensity scores.\n",
    "    dist : pandas.DataFrame\n",
    "        The distance matrix where rows correspond to treated units and columns correspond to control units.\n",
    "    with_replacement : bool, optional\n",
    "        Whether to match with replacement (default: True).\n",
    "    optimal : bool, optional\n",
    "        Whether to perform optimal matching (default: False). Creates optimal pairing when with_replacement = False.\n",
    "    num_matches : int, optional\n",
    "        Number of control matches per treatment unit. If 1, one-to-one matching is performed. If >1, one-to-many matching is performed.\n",
    "    caliper : float, optional\n",
    "        The caliper to be applied to the distance between treated and control units for the propensity score. If caliper is smaller, we ensure units matched have similar propensity scores.\n",
    "  \n",
    "    Returns:\n",
    "    --------\n",
    "    match_full : pandas.DataFrame\n",
    "        Dataframe with the matched units. The match_num column indicates the match number for each treated and control unit pair.\n",
    "\n",
    "    \"\"\"\n",
    "    # Identify treatment and control units\n",
    "    treatment = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "\n",
    "    matches = [] # Create an empty list to store matches\n",
    "\n",
    "    # -------- OPTION 1: WITH REPLACEMENT ----------\n",
    "    if with_replacement:\n",
    "        match_num = 1 # Initialize the match number\n",
    "        for t_idx, t_row in treatment.iterrows(): # Iterate over the treatment units\n",
    "            control_within_caliper = control[abs(PS.loc[t_idx] - PS.loc[control.index]) <= caliper] # Get the control units within the propensity score caliper\n",
    "            match_indices = dist.loc[t_idx][control_within_caliper.index].nsmallest(num_matches).index.tolist() # Get the indices of the k closest control units within the caliper\n",
    "            matches.extend([[t_idx, c_idx, match_num] for c_idx in match_indices]) # Add the matches to the matches list\n",
    "            match_num += 1 # Increment the match number\n",
    "        \n",
    "    # -------- OPTION 2: WITHOUT REPLACEMENT ----------\n",
    "    elif not with_replacement:\n",
    "        # --------- OPTION 2A: GREEDY MATCHING ----------\n",
    "        if not optimal:\n",
    "            match_num = 1 # Initialize the match number\n",
    "            matched_controls = set()  # Keep track of matched control units\n",
    "            for t_idx, t_row in treatment.iterrows(): # Iterate over the treatment units\n",
    "                available_controls = [c_idx for c_idx in control.index if c_idx not in matched_controls] # Safe the control units that have not been matched yet\n",
    "                control_within_caliper = control[abs(PS.loc[t_idx] - PS.loc[available_controls.index]) <= caliper] # Get the control units within the propensity score caliper from the available controls\n",
    "                control_indices = dist.loc[t_idx][control_within_caliper].nsmallest(num_matches).index.tolist() # Get the indices of the k closest control units within the caliper\n",
    "                matches.extend([[t_idx, c_idx, match_num] for c_idx in control_indices]) # Add the matches to the matches list\n",
    "                matched_controls.update(control_indices) # Add the matched control indices to the matched controls set to update the available controls\n",
    "                match_num += 1 # Increment the match number\n",
    "                \n",
    "        # --------  OPTION 2B: OPTIMAL MATCHING (OPTIMIZATION) ----------\n",
    "        if optimal:\n",
    "            # Create optimization problem: minimization problem\n",
    "            prob = pulp.LpProblem(\"Matching_Problem\", pulp.LpMinimize)\n",
    "\n",
    "            # Define decision variables: binary variable x_{ij} = 1 if treatment unit i is matched to control unit j, 0 otherwise\n",
    "            x = pulp.LpVariable.dicts(\"x\", [(t_idx, c_idx) for t_idx in treatment.index for c_idx in control.index], cat='Binary')\n",
    "\n",
    "            # Objective function: minimize total distance\n",
    "            prob += pulp.lpSum(dist.loc[t_idx][c_idx] * x[(t_idx, c_idx)] for t_idx in treatment.index for c_idx in control.index)\n",
    "\n",
    "            # Constraint 1: each treatment unit is matched to at least the number of control specified within the caliper\n",
    "            for t_idx in treatment.index:\n",
    "                controls_within_caliper = control[abs(PS.loc[t_idx] - PS.loc[control.index]) <= caliper] # Get the control units within the propensity score caliper\n",
    "                prob += pulp.lpSum(x[(t_idx, c_idx)] for c_idx in controls_within_caliper.index) >= num_matches \n",
    "            \n",
    "            # Constraint 2: each control unit is matched to at most one treatment unit\n",
    "            for c_idx in control.index:\n",
    "                prob += pulp.lpSum(x[(t_idx, c_idx)] for t_idx in treatment.index) <= 1\n",
    "\n",
    "            # Solve the optimization problem\n",
    "            start_time = time.time()  # Record the start time\n",
    "            prob.solve(pulp.GLPK_CMD(msg=0)) # Use GLPK solver and suppress output\n",
    "            end_time = time.time()  # Record the end time\n",
    "            execution_time = end_time - start_time # Calculate the execution time\n",
    "            print(\"Execution Time:\", execution_time, \"seconds\")\n",
    "\n",
    "            # Retrieve the matches from the decision variables \n",
    "            match_num = 1  # Initialize the match number\n",
    "            matched_treatment = set()  # Keep track of treatment units that have been matched\n",
    "            for t_idx in treatment.index: # Iterate over the treatment units\n",
    "                matched_control = []  # Keep track of control units matched with this specific treatment unit\n",
    "                for c_idx in control.index: # Iterate over the control units\n",
    "                    if pulp.value(x[(t_idx, c_idx)]) == 1: # If the decision variable is 1, the pair is a match\n",
    "                        matched_control.append(c_idx) # Add the control unit index to the matched control list\n",
    "                if matched_control: # If the matched control list is not empty\n",
    "                    if t_idx not in matched_treatment: # If the treatment unit has not been matched yet, it is not in the set\n",
    "                        for c_idx in matched_control: # Iterate over the matched control units to this specific treatment unit\n",
    "                            matches.append([t_idx, c_idx, match_num]) # Add the matched pair to the matches list with the match number\n",
    "                        matched_treatment.add(t_idx) # Add the treatment unit index to the matched treatment set to keep track of matched treatment units\n",
    "                        match_num += 1 # Increment the match number\n",
    "    \n",
    "    # If the problem found matches then return the matches\n",
    "    if matches:\n",
    "        match_full = pd.DataFrame(columns=X.columns)  # Create an empty dataframe to store the matches including the features from X\n",
    "\n",
    "        # Iterate over the matches and include the matched number column \n",
    "        for match in matches:\n",
    "            match_treatment = treatment.loc[match[0]].to_frame().T # Get matched treatment unit features values from X\n",
    "            match_control = control.loc[match[1]].to_frame().T # Get matched control unit features values from X\n",
    "            match_treatment['match_num'] = match[2]  # Use the match number from the matches list\n",
    "            match_control['match_num'] = match[2]  # Use the match number from the matches list\n",
    "            match_full = pd.concat([match_full, match_treatment, match_control]) # Concatenate the matched treatment and control units to the match_full dataframe\n",
    "                \n",
    "        # Add a column for the treatment indicator from T\n",
    "        match_full['treatment'] = T[match_full.index]\n",
    "\n",
    "        #drop duplicated treatment rows that are added for when num_matches>1 (just for formatting purposes)\n",
    "        match_full=match_full[~((match_full['treatment']==1) & (match_full['match_num'].duplicated()))]\n",
    "        return match_full\n",
    "    \n",
    "    # If the problem did not find matches, then print a suggestion to relax the constraints\n",
    "    elif not matches:\n",
    "        print('** WARNING ** No matches found. You should try to relax your constraints in the optimization problem as the problem may be infeasible. Also, you can try to reduce the number of control matches as there might not be enough control units to match to.')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_matching_PS_calipers_HC(X, T, PS, dist, with_replacement=True, optimal=False, num_matches=1, caliper=.02,\n",
    "                                 hard_constraints=None):\n",
    "    \"\"\"\n",
    "    Matches treated and control units based on the distance matrix and propensity scores with calipers including hard constraints in the optimization.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pandas.DataFrame\n",
    "        The feature matrix.\n",
    "    T : pandas.Series\n",
    "        Series with treatment indicator. 1 if treatment, 0 if control.\n",
    "    PS : pandas.Series\n",
    "        Series with propensity scores.\n",
    "    dist : pandas.DataFrame\n",
    "        The distance matrix where rows correspond to treated units and columns correspond to control units.\n",
    "    with_replacement : bool, optional\n",
    "        Whether to match with replacement (default: True).\n",
    "    optimal : bool, optional\n",
    "        Whether to perform optimal matching (default: False). Creates optimal pairing when with_replacement = False.\n",
    "    num_matches : int, optional\n",
    "        Number of control matches per treatment unit. If 1, one-to-one matching is performed. If >1, one-to-many matching is performed.\n",
    "    caliper : float, optional\n",
    "        The caliper to be applied to the distance between treated and control units for the propensity score. If caliper is smaller, we ensure units matched have similar propensity scores.\n",
    "    hard_constraints : list, optional\n",
    "        The list of features to be used for distance calculation with hard constraints (default: None). If None, no features have hard constraints.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    match_full : pandas.DataFrame\n",
    "        Dataframe with the matched units. The match_num column indicates the match number for each treated and control unit pair.\n",
    "\n",
    "    \"\"\"\n",
    "    # Identify treatment and control units\n",
    "    treatment = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "\n",
    "    matches = [] # Create an empty list to store matches\n",
    "\n",
    "    # -------- OPTION 1: WITH REPLACEMENT ----------\n",
    "    if with_replacement:\n",
    "        match_num = 1 # Initialize the match number\n",
    "        for t_idx, t_row in treatment.iterrows(): # Iterate over the treatment units\n",
    "            control_within_caliper = control[abs(PS.loc[t_idx] - PS.loc[control.index]) <= caliper] # Get the control units within the propensity score caliper\n",
    "            match_indices = dist.loc[t_idx][control_within_caliper.index].nsmallest(num_matches).index.tolist() # Get the indices of the k closest control units within the caliper\n",
    "            matches.extend([[t_idx, c_idx, match_num] for c_idx in match_indices]) # Add the matches to the matches list\n",
    "            match_num += 1 # Increment the match number\n",
    "        \n",
    "    # -------- OPTION 2: WITHOUT REPLACEMENT ----------\n",
    "    elif not with_replacement:\n",
    "        # --------- OPTION 2A: GREEDY MATCHING ----------\n",
    "        if not optimal:\n",
    "            match_num = 1 # Initialize the match number\n",
    "            matched_controls = set()  # Keep track of matched control units\n",
    "            for t_idx, t_row in treatment.iterrows(): # Iterate over the treatment units\n",
    "                available_controls = [c_idx for c_idx in control.index if c_idx not in matched_controls] # Safe the control units that have not been matched yet\n",
    "                control_within_caliper = control[abs(PS.loc[t_idx] - PS.loc[available_controls.index]) <= caliper] # Get the control units within the propensity score caliper from the available controls\n",
    "                control_indices = dist.loc[t_idx][control_within_caliper].nsmallest(num_matches).index.tolist() # Get the indices of the k closest control units within the caliper\n",
    "                matches.extend([[t_idx, c_idx, match_num] for c_idx in control_indices]) # Add the matches to the matches list\n",
    "                matched_controls.update(control_indices) # Add the matched control indices to the matched controls set to update the available controls\n",
    "                match_num += 1 # Increment the match number\n",
    "                \n",
    "        # --------  OPTION 2B: OPTIMAL MATCHING (OPTIMIZATION) ----------\n",
    "        if optimal:\n",
    "             # Create optimization problem: minimization problem\n",
    "            prob = pulp.LpProblem(\"Matching_Problem\", pulp.LpMinimize)\n",
    "\n",
    "            # Define decision variables: binary variable x_{ij} = 1 if treatment unit i is matched to control unit j, 0 otherwise\n",
    "            x = pulp.LpVariable.dicts(\"x\", [(t_idx, c_idx) for t_idx in treatment.index for c_idx in control.index], cat='Binary')\n",
    "\n",
    "            # Objective function: minimize total distance\n",
    "            prob += pulp.lpSum(dist.loc[t_idx][c_idx] * x[(t_idx, c_idx)] for t_idx in treatment.index for c_idx in control.index)\n",
    "\n",
    "            # Constraint 1: each treatment unit is matched to at least the number of control specified\n",
    "            for t_idx in treatment.index: \n",
    "                controls_within_caliper = control[abs(PS.loc[t_idx] - PS.loc[control.index]) <= caliper] # Get the control units within the propensity score caliper\n",
    "                controls_within_caliper_hc = [c_idx for c_idx in controls_within_caliper.index if all(X.loc[t_idx, f] == X.loc[c_idx, f] for f in hard_constraints)] # filter the controls by those where the hard constraint features have the same value for treated and control in X\n",
    "                prob += pulp.lpSum(x[(t_idx, c_idx)] for c_idx in controls_within_caliper_hc) >= num_matches\n",
    "\n",
    "            # Constraint 2: each control unit is matched to at most one treatment unit\n",
    "            for c_idx in control.index:\n",
    "                prob += pulp.lpSum(x[(t_idx, c_idx)] for t_idx in treatment.index) <= 1\n",
    "\n",
    "            # Solve the optimization problem\n",
    "            start_time = time.time()  # Record the start time\n",
    "            prob.solve(pulp.GLPK_CMD(msg=0)) # Use GLPK solver and suppress output\n",
    "            end_time = time.time()  # Record the end time\n",
    "            execution_time = end_time - start_time # Calculate the execution time\n",
    "            print(\"Execution Time:\", execution_time, \"seconds\")\n",
    "\n",
    "            # Retrieve the matches from the decision variables \n",
    "            match_num = 1  # Initialize the match number\n",
    "            matched_treatment = set()  # Keep track of treatment units that have been matched\n",
    "            for t_idx in treatment.index: # Iterate over the treatment units\n",
    "                matched_control = []  # Keep track of control units matched with this specific treatment unit\n",
    "                for c_idx in control.index: # Iterate over the control units\n",
    "                    if pulp.value(x[(t_idx, c_idx)]) == 1: # If the decision variable is 1, the pair is a match\n",
    "                        matched_control.append(c_idx) # Add the control unit index to the matched control list\n",
    "                if matched_control: # If the matched control list is not empty\n",
    "                    if t_idx not in matched_treatment: # If the treatment unit has not been matched yet, it is not in the set\n",
    "                        for c_idx in matched_control: # Iterate over the matched control units to this specific treatment unit\n",
    "                            matches.append([t_idx, c_idx, match_num]) # Add the matched pair to the matches list with the match number\n",
    "                        matched_treatment.add(t_idx) # Add the treatment unit index to the matched treatment set to keep track of matched treatment units\n",
    "                        match_num += 1 # Increment the match number\n",
    "    \n",
    "    # If the problem found matches then return the matches\n",
    "    if matches:\n",
    "        match_full = pd.DataFrame(columns=X.columns)  # Create an empty dataframe to store the matches including the features from X\n",
    "\n",
    "        # Iterate over the matches and include the matched number column \n",
    "        for match in matches:\n",
    "            match_treatment = treatment.loc[match[0]].to_frame().T # Get matched treatment unit features values from X\n",
    "            match_control = control.loc[match[1]].to_frame().T # Get matched control unit features values from X\n",
    "            match_treatment['match_num'] = match[2]  # Use the match number from the matches list\n",
    "            match_control['match_num'] = match[2]  # Use the match number from the matches list\n",
    "            match_full = pd.concat([match_full, match_treatment, match_control]) # Concatenate the matched treatment and control units to the match_full dataframe\n",
    "                \n",
    "        # Add a column for the treatment indicator from T\n",
    "        match_full['treatment'] = T[match_full.index]\n",
    "\n",
    "        #drop duplicated treatment rows that are added for when num_matches>1 (just for formatting purposes)\n",
    "        match_full=match_full[~((match_full['treatment']==1) & (match_full['match_num'].duplicated()))]\n",
    "        return match_full\n",
    "    \n",
    "    # If the problem did not find matches, then print a suggestion to relax the constraints\n",
    "    elif not matches:\n",
    "        print('** WARNING ** No matches found. You should try to relax your constraints in the optimization problem as the problem may be infeasible. Also, you can try to reduce the number of control matches as there might not be enough control units to match to.')\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asmd(data, features, T='treatment', perspective=['individual', 'global']):\n",
    "    '''\n",
    "    Calculates the average standardized mean difference for each feature. ASMD = Abs(Mean Treated - Mean Control)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: pd.DataFrame\n",
    "        The feature matrix with the treatment. \n",
    "    features: list\n",
    "        The list of important features to be used for the balance calculation.\n",
    "    T: str, optional\n",
    "        The name of the treatment column (default: 'treatment').\n",
    "    perspective: str\n",
    "        The perspective from which to calculate the ASMD. Options are 'individual' or 'global'. Individual calculates the ASMD for each match, and global calculates the ASMD for the entire dataset.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ASMD_by_feature: pd.Series\n",
    "        A series with the average standardized mean difference for each feature. \n",
    "    '''\n",
    "    # Identify treatment and control units    \n",
    "    treatment = data[data[T] == 1] \n",
    "    control = data[data[T] == 0] \n",
    "\n",
    "    # ----------------- OPTION 1: GLOBAL ASMD ------------------\n",
    "    if perspective=='global': # Calculate ASMD globally comparing treatment and control\n",
    "        ASMD_by_feature = abs(treatment[features].mean() - control[features].mean()) # calculate ASMD for each feature\n",
    "        ASMD_global=ASMD_by_feature.mean() # calculate global ASMD as the mean of the ASMDs for each feature\n",
    "        print(f'ASMD: {ASMD_global}')\n",
    "        return ASMD_by_feature\n",
    "    # ----------------- OPTION 2: INDIVIDUAL ASMD ------------------\n",
    "    elif perspective=='individual': # Calculate ASMD individually for each match \n",
    "        # Initialize an empty DataFrame to store the bias\n",
    "        asmd_by_match = pd.DataFrame(index=data['match_num'].unique(), columns=features)\n",
    "\n",
    "        # Calculate the bias for each feature within each match\n",
    "        for match_num in data['match_num'].unique():\n",
    "            match_treated = treatment[treatment['match_num'] == match_num]\n",
    "            match_control = control[control['match_num'] == match_num]\n",
    "\n",
    "            # Calculate the bias for each feature\n",
    "            match_asmd = np.abs(match_treated[features].mean() - match_control[features].mean()) #this is like ASMD by match\n",
    "\n",
    "            # Assign the bias values to the corresponding match and feature in the bias DataFrame\n",
    "            asmd_by_match.loc[match_num] = match_asmd\n",
    "        asmd_by_match.loc['ASMD_by_feature'] = asmd_by_match[features].mean() #calculate the mean ASMD for each feature across matches\n",
    "\n",
    "        print(f'ASMD by matches: {asmd_by_match.loc[\"ASMD_by_feature\"].mean()}')\n",
    "        return asmd_by_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_matches(M, T, matches, features=None):\n",
    "    \"\"\"\n",
    "    Evaluate the matching by calculating the bias between the treated and control units within each match, differentiated by features.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    M: pd.DataFrame\n",
    "        The feature matrix with the treatment and matches column.\n",
    "    T: str\n",
    "        The treatment variable column name.\n",
    "    matches: str\n",
    "        The column name with match identifier number for each pair or group of matched units.\n",
    "    features: list, optional\n",
    "        The list of features to calculate the bias (default: None). If None, all features are used.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    bias: pd.DataFrame\n",
    "        A DataFrame with the calculated bias for each match and feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify the treated and control units\n",
    "    treated = M[M[T] == 1]\n",
    "    control = M[M[T] == 0]\n",
    "\n",
    "    # Initialize an empty DataFrame to store the bias for the features\n",
    "    bias = pd.DataFrame(index=M[matches].unique(), columns=features)\n",
    "\n",
    "    # Calculate the bias for each feature within each match\n",
    "    for match_num in M[matches].unique(): # Iterate over the matches\n",
    "        match_treated = treated[treated[matches] == match_num] \n",
    "        match_control = control[control[matches] == match_num]\n",
    "\n",
    "        # Calculate the bias for each feature: the mean absolute difference between the treated and control units within each match\n",
    "        match_bias = np.abs(match_treated[features].values - match_control[features].values).mean(axis=0)\n",
    "\n",
    "        bias.loc[match_num] = match_bias # Assign the bias values to the corresponding match and feature in the bias DataFrame\n",
    "    bias['Total_bias']=bias.sum(axis=1) # Calculate the total bias for each match\n",
    "    bias.loc['Average_bias'] = bias[features].mean(axis=0) # Calculate the average bias for each feature\n",
    "\n",
    "    print(f'Average bias by features: {bias.loc[\"Average_bias\"].mean()}')\n",
    "    print(f'Average total distance: {bias[\"Total_bias\"].sum()}')\n",
    "    return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ate(X, matched_df, outcome='STAAR_Meets'):\n",
    "    \"\"\"\n",
    "    Calculate the average treatment effect (ATE) for a matched dataset.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pandas.DataFrame\n",
    "        Dataframe containing the features.\n",
    "    matched_df : pandas.DataFrame\n",
    "        Matched dataset with match numbers assigned to each unit.\n",
    "    \"\"\"\n",
    "    # Identify the treated and control units\n",
    "    mean_treated = X.iloc[matched_df[matched_df['treatment'] == 1].index][outcome].mean()\n",
    "    mean_control = X.iloc[matched_df[matched_df['treatment'] == 0].index][outcome].mean()\n",
    "\n",
    "    # Calculate the ATE\n",
    "    ate = mean_treated - mean_control\n",
    "    print(f'ATE: {ate}, Mean treated: {mean_treated}, Mean control: {mean_control}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_matches(M, T, matches, features):\n",
    "    \"\"\"\n",
    "    Calculate variance for each match and feature in a matched dataset.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    M: pd.DataFrame\n",
    "        The feature matrix with the treatment and matches column.\n",
    "    T: str\n",
    "        The treatment variable column name.\n",
    "    matches: str\n",
    "        The column name with match identifier number for each pair or group of matched units.\n",
    "    features : list\n",
    "        List of feature column names to evaluate variance and bias. \n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df_var: pd.DataFrame\n",
    "        A DataFrame with the variance for each match and feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify the treated and control units\n",
    "    treated = M[M[T] == 1]\n",
    "    control = M[M[T] == 0]\n",
    "\n",
    "    # Get unique match numbers\n",
    "    unique_matches = M['match_num'].unique()\n",
    "\n",
    "    # Empty dataframes to store variance by match and feature\n",
    "    df_var = pd.DataFrame(index=unique_matches, columns=features)\n",
    "\n",
    "    for m in unique_matches:\n",
    "        for f in features:\n",
    "            df_var.loc[m, f] = M.loc[M['match_num'] == m, f].var()\n",
    "            \n",
    "    #for each column, calculate the mean of the variance across matches\n",
    "    for i in df_var.columns:\n",
    "        df_var.loc['Mean_var_features', i]=df_var[i].mean()\n",
    "    \n",
    "    print(f'Average variance across matches: {df_var.iloc[-1, :-1].mean()}')\n",
    "    return df_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_boxplot(matching, treatment, match_num, features):\n",
    "\"\"\"\n",
    "This function creates a boxplot for each feature bias you want to compare.\n",
    "\n",
    "Parameters:\n",
    "------------\n",
    "matching: Dataframe\n",
    "\tThe matched data.\n",
    "treatment: str \n",
    "\tThe treatment variable column name.\n",
    "matches: str\n",
    "\tThe column name with match identifier number for each pair or group of matched units.\n",
    "features: list, optional\n",
    "\tThe list of features to calculate the bias (default: None). If None, all features are used.\n",
    "\"\"\"\n",
    "\n",
    "    bias_df = bias_matches(matching, treatment, match_num, features=features).iloc[:-1, :-1]\n",
    "    bias_df = bias_df.apply(pd.to_numeric, errors='coerce')\n",
    "    num_features = len(features)\n",
    "    num_plots_per_line = min(num_features, 5)\n",
    "    num_lines = -(-num_features // num_plots_per_line) # Ceiling division\n",
    "    fig, axes = plt.subplots(nrows=num_lines, ncols=num_plots_per_line, figsize=(15, 4*num_lines))\n",
    "    for i, line_axes in enumerate(axes):\n",
    "        for j, ax in enumerate(line_axes):\n",
    "        feature_idx = i * num_plots_per_line + j\n",
    "        if feature_idx < num_features:\n",
    "            feature = features[feature_idx]\n",
    "            ax.boxplot(bias_df[feature])\n",
    "            ax.set_title(feature, fontsize=8)\n",
    "            ax.set_xticklabels([])\n",
    "        else:\n",
    "            ax.axis('off') # Hide excess subplots\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_evaluation(matching, treatment, match_num, features):\n",
    "\"\"\"\n",
    "Plots the cumulative distribution of bias values for each feature.\n",
    "Parameters:\n",
    "------------\n",
    "matching: Dataframe\n",
    "\tThe matched data.\n",
    "treatment: str \n",
    "\tThe treatment variable column name.\n",
    "matches: str\n",
    "\tThe column name with match identifier number for each pair or group of matched units.\n",
    "features: list, optional\n",
    "\tThe list of features to calculate the bias (default: None). If None, all features are used.\n",
    "\"\"\"\n",
    "\n",
    "    bias_df = bias_matches(matching, treatment, match_num, features=features).iloc[:-1, :-1]\n",
    "    num_features = len(features)\n",
    "    num_plots_per_line = min(num_features, 5)\n",
    "    num_lines = -(-num_features // num_plots_per_line) # Ceiling division\n",
    "    fig, axes = plt.subplots(nrows=num_lines, ncols=num_plots_per_line, figsize=(15, 4*num_lines))\n",
    "    for i, line_axes in enumerate(axes):\n",
    "        for j, ax in enumerate(line_axes):\n",
    "        feature_idx = i * num_plots_per_line + j\n",
    "        if feature_idx < num_features:\n",
    "            feature = features[feature_idx]\n",
    "            cum_dist = bias_df[feature].value_counts().sort_index().cumsum() / bias_df.shape[0]\n",
    "            ax.plot(cum_dist.index, cum_dist.values)\n",
    "            ax.set_title(feature, fontsize=8)\n",
    "            ax.set_yticks(np.arange(0, 1.1, 0.25))\n",
    "        else:\n",
    "                ax.axis('off') # Hide excess subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9065f229f78de392276990b09d69cf52daf9e7892e0a6186d0873b2049073fcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
